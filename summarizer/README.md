# ğŸ“ Modular Text Summarization Pipeline (Self-hosted + Local Inference)

This project implements a **modular, class-based summarization pipeline** using a **self-hosted Transformer model** to summarize dialogue-based data (SAMSum dataset). The solution leverages models like T5, BART, and Pegasus â€” all running locally with no reliance on external APIs.

---

## ğŸ“Œ Project Highlights

- ğŸ’¬ Works on **dialogue summarization** using the [SAMSum dataset](https://huggingface.co/datasets/samsum)
- ğŸ§  Utilizes **state-of-the-art Transformer models** (`google/pegasus-large`)
- âš™ï¸ Built with **modular, class-based architecture**
- ğŸ§ª Includes ROUGE-based **evaluation**
- ğŸ§µ Captures model performance insights with comparisons
- ğŸ§± Easily extensible and GPU-compatible

---

## ğŸ§© Folder Structure

```
summarizer/
â”œâ”€â”€ data_loader.py     # Loads and splits dataset
â”œâ”€â”€ model.py           # Summarization model wrapper
â”œâ”€â”€ pipeline.py        # Inference orchestrator
â”œâ”€â”€ evaluator.py       # ROUGE evaluation
â”œâ”€â”€ main.py            # Entry point to run the pipeline
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ INSIGHTS.md        # Findings & observations
â””â”€â”€ README.md          # This file
```

---

## âš™ï¸ Installation

1. Clone the repository and create a virtual environment:

```bash
git clone <repo-url>
cd summarizer
python -m venv venv
venv\Scripts\activate   # or source venv/bin/activate on macOS/Linux
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. (Optional) If you have an NVIDIA GPU:

```bash
# Make sure PyTorch is installed with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

## ğŸš€ Running the Project

```bash
python main.py
```

This will:
- Load and preprocess the SAMSum dataset
- Initialize the selected model (default: `google/pegasus-large`)
- Run inference on a small sample
- Evaluate with ROUGE scores

---

## ğŸ§  Models Compared

| Model Name               | Inference Quality | Notes |
|--------------------------|-------------------|-------|
| `t5-small`               | âŒ Basic, shallow summaries |
| `facebook/bart-large-cnn`| ğŸŸ¡ Better fluency, weak abstraction |
| `google/pegasus-xsum`    | ğŸŸ¡ More abstract, some improvement |
| **`google/pegasus-large`** | âœ… Best quality + structure + ROUGE scores |

---

## ğŸ“Š Evaluation

Evaluation is done using **ROUGE-1**, **ROUGE-2**, and **ROUGE-L** metrics:

| Metric    | Score   |
|-----------|---------|
| ROUGE-1   | 0.3165  |
| ROUGE-2   | 0.1477  |
| ROUGE-L   | 0.2610  |

Evaluation was based on 10 randomly selected test samples from the SAMSum dataset.

---

## ğŸ” Sample Output

Example:
```
DIALOGUE:
Helen: Not this time. I need to finish the project. My boss will kill me if I put off the deadline once more.
Tricia: Can't you just pop in for one drink?

REFERENCE:
Helen is not going to the party because she needs to finish a project.

GENERATED SUMMARY:
I need to finish the project. My boss will kill me if I put off the deadline once more.
```

---

## ğŸ§© Design Choices

- Modular class-based structure for **clarity and maintainability**
- Evaluator module isolated to allow **plug-and-play metrics**
- `pipeline.py` used as orchestration layer for **data â†’ model â†’ evaluation**
- Fully local model execution, using **Hugging Face Transformers**

---

## ğŸ”® Future Improvements

- ğŸ§ª Fine-tuning Pegasus or BART on SAMSum for dialogue-specific learning
- ğŸ§µ Add speaker-tag processing for improved context understanding
- ğŸ›ï¸ Integrate hyperparameter tuning (beam size, length penalty)
- ğŸ“ˆ Add visualizations (ROUGE score trends, token lengths, etc.)


